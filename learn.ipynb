{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools, os, torch\n",
    "\n",
    "from sim import Scheduler, Random, Leitner\n",
    "from data_process import process_original, reduce_df, eval_thresh, reduce_lexemes\n",
    "from get_trajectory import get_traj\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data (clean if necesarry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/cleaned.csv\"):\n",
    "    process_original()\n",
    "    \n",
    "\n",
    "df = pd.read_csv(\"data/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning_language\n",
       "0    1452597\n",
       "1    5014791\n",
       "2    3407689\n",
       "3    1873734\n",
       "4     793935\n",
       "5     311480\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_map = {'de' : 0, 'en': 1, 'es': 2, 'fr': 3, 'it': 4, 'pt': 5}\n",
    "l_map = pd.read_csv(\"data/lexeme_map.csv\")\n",
    "\n",
    "df.groupby('learning_language').count().loc[:, 'user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 5 million english items, 3 million Spanish 1.9 million French and 1.4 million German. Italian and Portugese each have hundreds of thousands. It would be useful to restrict out studies to just the English users so we reduce the dimensionality of our action and state spaces.\n",
    "\n",
    "There are 43.8 thousand learners (trajectories) we have to provide our RL agents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['learning_language'] == 1].copy()\n",
    "df = df.drop(['learning_language'], axis=1)\n",
    "reduce_df(df)\n",
    "df.loc[:, 'difficulty'] = df.loc[:, 'difficulty'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2983 lexemes\n"
     ]
    }
   ],
   "source": [
    "english_counts = df.groupby('lexeme_id').count().loc[:, 'timestamp']\n",
    "n_lex = len(english_counts)\n",
    "print(f\"There are {n_lex} lexemes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold 500 there are 24.81% lexemes above and 75.19% below\n",
      "\n",
      "There would be 94.46% of data included and 5.54% of data excluded\n"
     ]
    }
   ],
   "source": [
    "n_items = int(500)\n",
    "\n",
    "eval_thresh(df, english_counts, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_lex , lex_to_idx = {}, {}\n",
    "df, included = reduce_lexemes(df, n_items)\n",
    "\n",
    "get_traj(df, included)\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# for item in included:\n",
    "#     idx_to_lex[i] = item\n",
    "#     lex_to_idx[item] = i\n",
    "#     i += 1\n",
    "\n",
    "\n",
    "\n",
    "# df = df.sort_values(by=['user_id', 'timestamp'])\n",
    "\n",
    "# df_first_lex = df.groupby('lex_user').head(1)\n",
    "# max_sess = df.groupby('user_id').max().loc[:, 'session']\n",
    "# min_sess = df.groupby('user_id').min().loc[:, 'session']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {}\n",
    "actions = {}\n",
    "\n",
    "\n",
    "itr = max_sess.items()\n",
    "itr2 = min_sess.items()\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        usr, mx = next(itr)\n",
    "        _, mn = next(itr2)\n",
    "        \n",
    "        sessions = int(mx - mn)\n",
    "        states[usr] = np.zeros((sessions + 1, len(included) * 3))\n",
    "        actions[usr] = np.zeros((sessions + 1, len(included)))\n",
    "    except:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in df_first_lex.itertuples(index=False):\n",
    "    sess, usr, lex = r.session, r.user_id, r.lexeme_id\n",
    "    \n",
    "    h_seen, h_corr = r.history_seen, r.history_correct\n",
    "    s_seen = r.session_seen\n",
    "    \n",
    "    c = lex_to_idx[lex]\n",
    "    c_s = c * 3\n",
    "    \n",
    "    states[usr][0, c_s] = h_seen\n",
    "    states[usr][0, c_s+1] = h_corr\n",
    "    try:\n",
    "        states[usr][1, c_s+2] = -1  \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    actions[usr][0, c] = s_seen\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_arr = np.array([0] * len(states[3][0, :]))\n",
    "\n",
    "for i in range(len(add_arr)):\n",
    "    if i % 3 == 2:\n",
    "        add_arr[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_usr = None\n",
    "l_sess = None\n",
    "for r in df.itertuples(index=True):\n",
    "    usr, sess, lex, s_seen, s_corr = r.user_id, r.session, \\\n",
    "        r.lexeme_id, r.session_seen, r.session_correct\n",
    "    \n",
    "    m_sess, ma_sess = min_sess[usr], max_sess[usr]\n",
    "\n",
    "    \n",
    "    c = lex_to_idx[lex]\n",
    "    c_s = c * 3\n",
    "    row = sess - m_sess\n",
    "    \n",
    "    actions[usr][0, c] = s_seen\n",
    "    \n",
    "    \n",
    "    if sess != l_sess:\n",
    "        l_sess = sess\n",
    "        states[usr][row, :] = np.copy(states[usr][row - 1, :]) + add_arr   \n",
    "    \n",
    "    if sess != ma_sess:\n",
    "        states[usr][row + 1, c_s] += s_seen\n",
    "        states[usr][row + 1, c_s +1] += s_corr\n",
    "        #Set to -1 so when we add 1 to it it goes back to 0\n",
    "        states[usr][row + 1, c_s + 2] = -1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_traj(df, in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
