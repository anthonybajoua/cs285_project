{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"learn.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNT5yyM2BtsO","executionInfo":{"status":"ok","timestamp":1607810850939,"user_tz":480,"elapsed":782,"user":{"displayName":"Anthony Bajoua","photoUrl":"","userId":"06381645334437819044"}},"outputId":"37c0377c-815d-4a8c-a73a-410da63ee5ea"},"source":["from os.path import exists\n","\n","if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","\n","  if not exists('/content/drive/MyDrive'):\n","    drive.mount('/content/drive/')\n","\n","  if not exists('/content/drive/MyDrive/cs285_project'):\n","    !git pull https://github.com/anthonybajoua/cs285_project.git\n","\n","\n","  %cd /content/drive/MyDrive/cs285_project\n","else:\n","  pass"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/cs285_project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTuq9jKD_qjx","executionInfo":{"status":"ok","timestamp":1607810962600,"user_tz":480,"elapsed":1304,"user":{"displayName":"Anthony Bajoua","photoUrl":"","userId":"06381645334437819044"}},"outputId":"4476fae3-6e4a-4232-a28d-e394aaf2aebc"},"source":["!git remote"],"execution_count":7,"outputs":[{"output_type":"stream","text":["origin\thttps://github.com/anthonybajoua/cs285_project.git (fetch)\n","origin\thttps://github.com/anthonybajoua/cs285_project.git (push)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rhdNkDVOEmB8"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import itertools, os, torch\n","from torch import nn\n","\n","from sim import Scheduler, Random, Leitner\n","from data_process import process_original, reduce_df, eval_thresh, reduce_lexemes, normalize\n","from get_trajectory import trajectory_generator, makeSingle\n","\n","import torch.nn as nn\n","import torch\n","import random\n","import cs285.infrastructure.pytorch_util as ptu\n","from tqdm.notebook import trange, tqdm\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","ptu.init_gpu()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTnvkbNIBtsQ"},"source":["## Load data (clean if necesarry)"]},{"cell_type":"code","metadata":{"id":"IkHyBG9lBtsR"},"source":["if not os.path.exists(\"data/cleaned.csv\"):\n","    process_original()\n","\n","df = pd.read_csv(\"data/cleaned.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m9Dz-uUhBtsR"},"source":["lang_map = {'de' : 0, 'en': 1, 'es': 2, 'fr': 3, 'it': 4, 'pt': 5}\n","l_map = pd.read_csv(\"data/lexeme_map.csv\")\n","\n","df.groupby('learning_language').count().loc[:, 'user_id']\n","\n","df = df.loc[df['learning_language'] == 1]\n","df = df.drop(['learning_language'], axis=1)\n","reduce_df(df)\n","normalize(df,'difficulty')\n","\n","\n","english_counts = df.groupby('lexeme_id').count().loc[:, 'timestamp']\n","n_lex = len(english_counts)\n","print(f\"There are {n_lex} lexemes\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PGKPZysQBtsR"},"source":["We have about 5 million english items, 3 million Spanish 1.9 million French and 1.4 million German. Italian and Portugese each have hundreds of thousands. It would be useful to restrict out studies to just the English users so we reduce the dimensionality of our action and state spaces.\n","\n","There are 43.8 thousand learners (trajectories) we have to provide our RL agents.\n","\n"]},{"cell_type":"code","metadata":{"id":"ako_T0hgBtsR"},"source":["n_items = int(5000)\n","\n","eval_thresh(df, english_counts, n_items)\n","\n","df, included = reduce_lexemes(df, n_items)\n","\n","tg = trajectory_generator(df, included)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Eq4MUodA4D4"},"source":["## Run Behavioral Cloning (singular input case)"]},{"cell_type":"code","metadata":{"id":"yeC6iPzgBtsS"},"source":["def createMLP(inSize, outSize, hidSize, nHidden, activation=nn.ReLU()):\n","    '''\n","    inSize - input size\n","    outSize - output size\n","    hidSize - hidden layer size\n","    nHidden - number hidden layers\n","    '''\n","    activation = nn.ReLU\n","    bc_loss = nn.MSELoss()\n","    layers = [nn.Linear(inSize, hidSize)]\n","    for i in range(nHidden):\n","        layers.append(nn.Linear(hidSize, hidSize))\n","        layers.append(activation())\n","    layers.append(nn.Linear(hidSize, outSize))\n","    net = nn.Sequential(*layers)\n","    return net\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mh8Bf94YlueM"},"source":["def trainNetwork(net, data, included, epochs, \\\n","                 nTraj=500, lr=1e-3, lossFn=nn.MSELoss(), norm=True):\n","  '''\n","  Train a BC neural net on data with included lexemes for epochs.\n","  '''\n","  tL, vL = [], []\n","\n","  net = net.to('cuda')\n","  opt = torch.optim.Adam(net.parameters(), lr=lr)\n","\n","  for _ in range(epochs):\n","\n","    tg = trajectory_generator(data, included, nTraj=nTraj)\n","\n","    val = False\n","\n","    for states, actions, _, _, _ in tg:\n","      \n","      states, actions = makeSingle(states, actions)\n","\n","      stateMat = np.vstack(list(states.values()))\n","      actionMat = np.vstack(list(actions.values()))\n","\n","      states = ptu.from_numpy(stateMat)\n","\n","      if norm:\n","        states = (states - states.mean(dim=0))/states.std(dim=0)\n","\n","      actions = ptu.from_numpy(actionMat)\n","\n","      if not val:\n","        valStates, valActions = states, actions\n","        val = True\n","\n","      \n","      preds = net.forward(states)\n","      loss = lossFn(preds, actions)\n","      valLoss = lossFn(valActions, net.forward(valStates))\n","\n","      print(\"\\n\\n\")\n","      print(torch.mean(actions).item(), torch.max(actions).item())\n","      print(loss.item(), valLoss.item())\n","      print(torch.mean(preds).item(), torch.max(preds).item(), torch.min(preds).item())\n","\n","      print(torch.sum(preds > .5).item(), torch.sum(actions > .5).item())\n","\n","      opt.zero_grad()\n","      loss.backward()\n","      opt.step()\n","\n","      tL.append(loss.item())\n","      vL.append(valLoss.item())\n","  return tL, vL\n","\n","net = createMLP(4, 1, 32, 2)\n","torch.cuda.empty_cache()\n","tL, eL = trainNetwork(net, df, included, 2, lr=1e-3, lossFn=nn.MSELoss())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qr5ZHsCdBtsS"},"source":["## Run Behavioral Cloning"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"4lc8Rp9uBtsS"},"source":["traj_idxs = list(states.keys())\n","def sample_traj(states, actions, idxs, batch_size):\n","    first_idx = idxs.pop()\n","    states_cat = states[first_idx]\n","    actions_cat = actions[first_idx]\n","    while states_cat.shape[0] < batch_size and len(idxs) > 0:\n","        new_idx = idxs.pop()\n","        states_cat = np.concatenate((states_cat, states[new_idx]))\n","        actions_cat = np.concatenate((actions_cat, actions[new_idx]))\n","    return states_cat, actions_cat\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g4UD6RpTBtsS"},"source":["state1, act1 = sample_traj(states, actions, [traj_idxs[0]], 1)\n","input_size = state1.shape[1]\n","output_size = act1.shape[1]\n","hidden_size = 3000\n","hidden_num = 2\n","activation = nn.ReLU\n","bc_loss = nn.MSELoss()\n","layers = [nn.Linear(input_size, hidden_size), activation()]\n","for i in range(hidden_num-1):\n","    layers.append(nn.Linear(hidden_size, hidden_size))\n","    layers.append(activation())\n","layers.append(nn.Linear(hidden_size, output_size))\n","bc_nn = nn.Sequential(*layers)\n","optimizer = torch.optim.Adam(bc_nn.parameters(), lr=1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25wogLr8BtsS"},"source":["train_iters = 2\n","train_losses = []\n","val_losses = []\n","batch_size = 200\n","val_batch_size = 200\n","split = int(np.floor(.9*len(traj_idxs)))\n","train_idxs = traj_idxs[:split]\n","val_idxs = traj_idxs[split:]\n","batches_per_epoch = split // batch_size\n","\n","for i in tqdm(range(train_iters)):\n","    idxs = train_idxs[:]\n","    random.shuffle(idxs)\n","    val_shuffled = val_idxs[:]\n","    random.shuffle(val_shuffled)\n","    for j in tqdm(range(batches_per_epoch)):\n","        optimizer.zero_grad()\n","        s, a = sample_traj(states, actions, idxs[j*batch_size:(j+1)*batch_size], batch_size)\n","        result = bc_nn(ptu.from_numpy(s))\n","        loss = bc_loss(result, ptu.from_numpy(a))\n","        loss.backward()\n","        optimizer.step()\n","        train_losses.append(loss.item())\n","        val_s, val_a = sample_traj(states, actions, val_shuffled[:], val_batch_size)\n","        val_losses.append(np.mean(np.square(ptu.to_numpy(bc_nn(ptu.from_numpy(val_s))) - val_a)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hy6CL68UBtsU"},"source":["plt.plot(train_losses[0:500], label='training')\n","plt.plot(val_losses[0:500], label='validation')\n","plt.yscale(\"log\")\n","plt.title(\"MSE Of Actual vs Predicted Action (1100 lexemes)\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lloEhQbeBtsU"},"source":["actions[3][0, :].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HwqXEYgBtsU"},"source":["min(bc_nn.forward(s))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQrLvCGsBtsU"},"source":[""],"execution_count":null,"outputs":[]}]}