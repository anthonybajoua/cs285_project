{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from collections import deque \n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Union\n",
    "from torch import optim\n",
    "from torch import distributions\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_session(x):\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    timestamps_sorted = np.array(sorted(list(set(x['timestamp']))))    \n",
    "    \n",
    "    result['timestamp'] = timestamps_sorted\n",
    "    result['session'] = list(range(len(timestamps_sorted)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'session_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'session_x'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f0c984c579ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_lex_hash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timestamp_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sess_diff'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'session_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2891\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'session_x'"
     ]
    }
   ],
   "source": [
    "lang_map = {'de' : 0, 'en': 1, 'es': 2, 'fr': 3, 'it': 4, 'pt': 5}\n",
    "\n",
    "if not os.path.exists(\"data/cleaned.csv\"):\n",
    "    df = pd.read_csv(\"data/settles.acl16.learning_traces.13m.csv\")\n",
    "\n",
    "    \n",
    "\n",
    "    #Hash lexemes for smaller storage\n",
    "    df['lexeme_id'] = df['lexeme_id'].apply(hash) % 1000000\n",
    "    \n",
    "    #Hash user id's for smaller storage\n",
    "    df['user_id'] = df['user_id'].apply(hash) % 5000000\n",
    "    \n",
    "    #Map languages to numbers for smaller storage\n",
    "    df['learning_language'] = df['learning_language'].map(lang_map)\n",
    "    df['ui_language'] = df['ui_language'].map(lang_map)\n",
    "    \n",
    "    \n",
    "    df['lexeme_string'] = df.lexeme_string.map(lambda x: x[0: x.find('<')])\n",
    "    \n",
    "    \n",
    "    df_small = df.loc[:, ['lexeme_id', 'lexeme_string']]\n",
    "    df_small = df_small.drop_duplicates()\n",
    "    df_small.to_csv(\"lexeme_map.csv\", index=False)\n",
    "    \n",
    "    #Drop this column as it's inferred from last two\n",
    "    df = df.drop([\"p_recall\", \"lexeme_string\"], axis=1)\n",
    "    \n",
    "    #This table contains the item difficulties of each lexeme\n",
    "    item_difficulties = df.groupby('lexeme_id').apply(lambda x: x['history_correct'].sum() / x['history_seen'].sum())\n",
    "\n",
    "    #This table contains user_id, timestamp, and which session the timestamp corresponds to\n",
    "    timestamp_map = df.loc[:, ['user_id', 'timestamp']].groupby(['user_id']).apply(timestamp_to_session)\n",
    "    timestamp_map = timestamp_map.reset_index().drop(['level_1'], axis = 1).loc[:, ['timestamp', 'user_id', 'session']]\n",
    "\n",
    "\n",
    "    \n",
    "    #Get session for each one\n",
    "    df = pd.merge(df, timestamp_map,  how='left', \\\n",
    "                                left_on=['user_id','timestamp'], right_on = ['user_id','timestamp'])\n",
    "\n",
    "    #Get difficulty for each one\n",
    "    df = pd.merge(df, pd.DataFrame(item_difficulties),  how='left', \\\n",
    "                                left_on=['lexeme_id'], right_on = ['lexeme_id'])\n",
    "\n",
    "    df = df.rename(columns={0: \"difficulty\"})\n",
    "\n",
    "    #Hash user and lexeme\n",
    "    df['user_lex_hash'] = \\\n",
    "            pd.Series(df.loc[:, ['user_id', 'lexeme_id']].astype(str).values.sum(axis=1)).apply(hash)\n",
    "    \n",
    "    \n",
    "    #Get the minimum timestamp for user lex hashes and merge the tables\n",
    "    min_times_per_user = \\\n",
    "            df.loc[:, ['user_lex_hash', 'timestamp']].groupby('user_lex_hash').min()\n",
    "\n",
    "    df = pd.merge(df, pd.DataFrame(min_times_per_user),  how='left', \\\n",
    "                                left_on=['user_lex_hash'], right_on = ['user_lex_hash'])\n",
    "    \n",
    "    #Sort and take the diff\n",
    "    df.sort_values(by=['user_lex_hash', 'timestamp_x'], inplace=True)\n",
    "    \n",
    "    df['sess_diff'] = df['session'].diff()\n",
    "\n",
    "\n",
    "    df = df.loc[df['timestamp_x'] != df['timestamp_y']]\n",
    "\n",
    "\n",
    "    for c in df.columns:\n",
    "            if c != 'lexeme_string':\n",
    "                df.loc[:, c] = pd.to_numeric(df[c], downcast='unsigned')\n",
    "\n",
    "    df.to_csv(\"data/cleaned.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    df = pd.read_csv(\"data/cleaned.csv\")\n",
    "    for c in df.columns:\n",
    "        if c != 'lexeme_string':\n",
    "            df[c] = pd.to_numeric(df[c], downcast='unsigned')\n",
    "            \n",
    "    lexeme_map = pd.read_csv(\"data/lexeme_map.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    \"\"\"\n",
    "    Parent class of any learning scheduler method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_items):\n",
    "        pass\n",
    "    \n",
    "    def next_item(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self, item, outcome):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class Random(Scheduler):\n",
    "    \"\"\"\n",
    "    Scheduler that selects random items to present.\n",
    "    \"\"\"\n",
    "    def __init(self, num_items):\n",
    "        self.n = num_items\n",
    "    \n",
    "    def next_item(self):\n",
    "        return np.random.randint(0, num_items)\n",
    "    \n",
    "    def update(self, item, outcome):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "\n",
    "class Leitner(Scheduler): \n",
    "    \"\"\"\n",
    "    This class implements a Leitner scheduler that samples from \n",
    "    boxes with exponentially decreasing probability. Cards enter\n",
    "    in box 0 and leave when they are correctly answered after entering \n",
    "    the final box\n",
    "    \"\"\"\n",
    "    def __init__(self, nb):\n",
    "        '''\n",
    "        :param nb: Number of boxes\n",
    "        boxes is a list of queues representing the boxes.\n",
    "        dist_boxes is sampling distribution for which box to select fromr\n",
    "        cards is a set of items in the boxes currently.\n",
    "        '''\n",
    "        self.boxes = [deque() for _ in nb]\n",
    "        self.dist_boxes = np.array([1/2**i for i in range(nb)]) / sum([1/2**i for i in range(nb)])\n",
    "        self.cards = set()\n",
    "        \n",
    "    \n",
    "    def next_item(self):\n",
    "        \"\"\"\n",
    "        Gets the next item in the learning sequence.\n",
    "        \"\"\"\n",
    "        self.recent_box = np.random.multinomial(1, self.dist_boxes).argmax()\n",
    "        \n",
    "        if len(self.boxes[self.recent_box]):\n",
    "            return self.boxes[self.recent_box].pop()\n",
    "        else:\n",
    "            return self.next_item()\n",
    "    \n",
    "    def update(self, item, outcome, thresh=.9):\n",
    "        \"\"\"\n",
    "        Updates the most recent item from the sequence\n",
    "        by putting it back depending on the outcome.\n",
    "        \"\"\"\n",
    "        if outcome > thresh:\n",
    "            new_box = self.recent_box + 1\n",
    "            if new_box >= len(self.boxes):\n",
    "                self.cards.remove(item)\n",
    "            else:\n",
    "                self.boxes[new_box].appendleft(item)\n",
    "        else:\n",
    "            new_box = max(self.recent_box - 1, 0)    \n",
    "            self.boxes[new_box].appendleft(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 5 million english learners, 3 million Spanish 1.9 million French and 1.4 million German learners. Italian and Portugese each have hundreds of thousands. It would be useful to restrict out studies to just the English users so we reduce the dimensionality of our dataset.\n",
    "\n",
    "Interestingly this dataset doesn't contain any Germans learning English so our studies will consist of using the Spanish, French and Italians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get session for each one\n",
    "# df = pd.merge(df, timestamp_map,  how='left', \\\n",
    "#                             left_on=['user_id','timestamp'], right_on = ['user_id','timestamp'])\n",
    "\n",
    "# #Get difficulty for each one\n",
    "# df = pd.merge(df, pd.DataFrame(item_difficulties),  how='left', \\\n",
    "#                             left_on=['lexeme_id'], right_on = ['lexeme_id'])\n",
    "\n",
    "# df = df.rename(columns={0: \"difficulty\"})\n",
    "\n",
    "# #Hash user and lexeme\n",
    "# df['user_lex_hash'] = \\\n",
    "#         pd.Series(df.loc[:, ['user_id', 'lexeme_id']].astype(str).values.sum(axis=1)).apply(hash)\n",
    "\n",
    "\n",
    "# #Get the minimum timestamp for user lex hashes and merge the tables\n",
    "# min_times_per_user = \\\n",
    "#         df.loc[:, ['user_lex_hash', 'timestamp']].groupby('user_lex_hash').min()\n",
    "\n",
    "# df = pd.merge(df, pd.DataFrame(min_times_per_user),  how='left', \\\n",
    "#                             left_on=['user_lex_hash'], right_on = ['user_lex_hash'])\n",
    "\n",
    "# Sort and take the diff\n",
    "\n",
    "df.sort_values(by=['user_lex_hash', 'timestamp_x'], inplace=True)\n",
    "\n",
    "df['sess_diff'] = df['session_x'].diff()\n",
    "\n",
    "\n",
    "df = df.loc[df['timestamp_x'] != df['timestamp_y']]\n",
    "\n",
    "\n",
    "for c in df.columns:\n",
    "        if c != 'lexeme_string':\n",
    "            df[c] = pd.to_numeric(df[c], downcast='unsigned')\n",
    "\n",
    "df.to_csv(\"data/cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add session count and difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the difficulties of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user we will define a trajectory where states are $N \\times 3$ where $N$ is the number of lexemes in our target dataset, and for each lexeme there's a tuple of (times_seen, times_correct, $\\nabla$) where $\\nabla$ is the time since the item was last seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = df_english.groupby('lexeme_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english.sort_values(by=['user_id', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation = Union[str, nn.Module]\n",
    "\n",
    "_str_to_activation = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'tanh': nn.Tanh(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'selu': nn.SELU(),\n",
    "    'softplus': nn.Softplus(),\n",
    "    'identity': nn.Identity(),\n",
    "}\n",
    "\n",
    "def build_mlp(\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        n_layers: int,\n",
    "        size: int,\n",
    "        activation: Activation = 'relu',\n",
    "        output_activation: Activation = 'identity') -> nn.Module:\n",
    "    \n",
    "    activation = _str_to_activation[activation]\n",
    "    \n",
    "    layers = [nn.Linear(input_size, size), activation]\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(nn.Linear(size, size))\n",
    "        layers.append(activation)\n",
    "\n",
    "    layers.append(nn.Linear(size, output_size))\n",
    "    net = nn.Sequential(*layers)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def from_numpy(*args, **kwargs):\n",
    "    return torch.from_numpy(*args, **kwargs).float().to(device)\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.to('cpu').detach().numpy()\n",
    "\n",
    "\n",
    "def init_gpu(use_gpu=True, gpu_id=0):\n",
    "    global device\n",
    "    if torch.cuda.is_available() and use_gpu:\n",
    "        device = torch.device(\"cuda:\" + str(gpu_id))\n",
    "        print(\"Using GPU id {}\".format(gpu_id))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not detected. Defaulting to CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Policy:\n",
    "    def __init__(self, ac_dim, ob_dim, n_layers, size, lr, **kwargs):\n",
    "        self.ob_dim = ob_dim\n",
    "        self.ac_dim = ac_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.size = size\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        self.mean_net = build_mlp(\n",
    "            input_size=self.ob_dim,\n",
    "            output_size=self.ac_dim,\n",
    "            n_layers=self.n_layers, size=self.size,\n",
    "        )\n",
    "        \n",
    "        self.logstd = nn.Parameter(\n",
    "            torch.zeros(self.ac_dim, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            itertools.chain([self.logstd], self.mean_net.parameters()),\n",
    "            self.learning_rate\n",
    "        )\n",
    "\n",
    "    def update(self, observations, actions):\n",
    "\n",
    "        dist = self.forward(from_numpy(observations))\n",
    "\n",
    "        expert = torch.tensor(actions)\n",
    "\n",
    "        acs_net = dist.rsample()\n",
    "        loss = self.loss(expert, acs_net)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "        \n",
    "    def run_training(self, n_iters, batch, all_obs, all_acs):\n",
    "            losses = []\n",
    "            for _ in range(n_iters):\n",
    "                n = np.random.randint(0, len(df))\n",
    "                \n",
    "                obs_curr, acs_curr = obs[n:n+batch], acs[n:n+batch]\n",
    "                loss_curr = self.update(obs, acs)\n",
    "                print(loss_curr)\n",
    "            return losses\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-357bc5fff90e>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[c] = pd.to_numeric(df[c], downcast='unsigned')\n"
     ]
    }
   ],
   "source": [
    "df['sess_diff'] = df['session'].diff()\n",
    "\n",
    "\n",
    "df = df.loc[df['timestamp_x'] != df['timestamp_y']]\n",
    "\n",
    "\n",
    "for c in df.columns:\n",
    "        if c != 'lexeme_string':\n",
    "            df.loc[:, c] = pd.to_numeric(df[c], downcast='unsigned')\n",
    "\n",
    "df.to_csv(\"data/cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
